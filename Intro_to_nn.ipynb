{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbLHTNfSclli"
      },
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtOYB-RHfc_r"
      },
      "source": [
        "1. Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "KjoeaDrk6fO7"
      },
      "outputs": [],
      "source": [
        "t_inputs = torch.from_numpy(inputs)\n",
        "t_targets = torch.from_numpy(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 73.,  67.,  43.],\n",
              "        [ 91.,  88.,  64.],\n",
              "        [ 87., 134.,  58.],\n",
              "        [102.,  43.,  37.],\n",
              "        [ 69.,  96.,  70.]])"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKzbJKfOgGV8"
      },
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "aXhKw6Tdj1-d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x129072510>"
            ]
          },
          "execution_count": 178,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.random.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "eApcB7eb6h9o"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True),\n",
              " tensor([0.6213], requires_grad=True))"
            ]
          },
          "execution_count": 179,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w = torch.randn(1, 3, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "w, b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYGxNGTaf5s6"
      },
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "pSz2j4Fh6jBv"
      },
      "outputs": [],
      "source": [
        "def model(t_inputs, w, b):\n",
        "    z = t_inputs@torch.t(w) + b\n",
        "    return 1/(1+torch.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "predicted_probs = model(t_inputs, w, b)\n",
        "print(predicted_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "На рандомних значеннях w та b результат кожного рядочка дуже близький до 1. Це пов\"язане з вибором ваг"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      },
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "1bWlovvx6kZS"
      },
      "outputs": [],
      "source": [
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "    log1 = true_labels*torch.log(predicted_probs)\n",
        "    log2 = (1-true_labels)*torch.log(1-predicted_probs)\n",
        "    entropies = -(log1 + log2)\n",
        "    return torch.mean(entropies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss = binary_cross_entropy(predicted_probs, t_targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFKpQxdHi1__"
      },
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "YAbXUNSJ6mCl"
      },
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[nan, nan, nan]])\n",
            "tensor([nan])\n"
          ]
        }
      ],
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDN1t1RujQsK"
      },
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "Пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOPSQyttpVjO"
      },
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "outputs": [],
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "-JwXiSpX6orh"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True),\n",
              " tensor([0.0006], requires_grad=True))"
            ]
          },
          "execution_count": 210,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "predicted_probs = model(t_inputs, w, b)\n",
        "print(predicted_probs)\n",
        "loss = binary_cross_entropy(predicted_probs, t_targets)\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ -5.4417, -18.9853, -10.0682]]) tensor([-0.0794])\n"
          ]
        }
      ],
      "source": [
        "print(w.grad, b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCdi44IT334o"
      },
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "mObHPyE06qsO"
      },
      "outputs": [],
      "source": [
        "def grad_desc(inputs, targets,  model, w, b, alpha, epochs):\n",
        "    epoch = 0\n",
        "    loss_threshold = 0.03\n",
        "    loss_item = float('inf')\n",
        "    while epoch < epochs and loss_item > loss_threshold:\n",
        "        predicted_probs = model(inputs, w, b)\n",
        "        loss = binary_cross_entropy(predicted_probs, targets)\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            w -= w.grad * alpha\n",
        "            b -= b.grad * alpha\n",
        "            w.grad.zero_()\n",
        "            b.grad.zero_()\n",
        "        epoch += 1\n",
        "        loss_item = loss.item()\n",
        "        if epoch % 100 == 0:\n",
        "            print(f'Epoch {epoch}, loss = {loss.item()}')\n",
        "        \n",
        "    print(f'Final loss:{loss.item()}')\n",
        "    return {'w': w, 'b': b}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100, loss = 0.03297306224703789\n",
            "Epoch 200, loss = 0.03153730183839798\n",
            "Epoch 300, loss = 0.03021698258817196\n",
            "Final loss:0.029990550130605698\n"
          ]
        }
      ],
      "source": [
        "epochs = 1000\n",
        "alpha = 1e-3\n",
        "params = grad_desc(t_inputs, t_targets, model, w, b, alpha, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[-0.4359252154827118, 0.1179201751947403, 0.5006595253944397]]"
            ]
          },
          "execution_count": 238,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params['w'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final values are \n",
            "weights : [[-0.4359252154827118, 0.1179201751947403, 0.5006595253944397]], \n",
            "bias : -0.02110949717462063\n"
          ]
        }
      ],
      "source": [
        "print(f'Final values are \\nweights : {params[\"w\"].tolist()}, \\nbias : {params[\"b\"].item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Запустила градієнтний спуск кілька разів, вважаю, що отримала мінімальне значення loss 0.02 (далі воно провалюється в nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuRhlyF9qAia"
      },
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X2dV30KtAPu"
      },
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "chrvMfBs6vjo"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([73., 67., 43.]), tensor([0.]))\n",
            "(tensor([91., 88., 64.]), tensor([1.]))\n",
            "(tensor([ 87., 134.,  58.]), tensor([1.]))\n"
          ]
        }
      ],
      "source": [
        "t_inputs = torch.from_numpy(inputs)\n",
        "t_targets = torch.from_numpy(targets)\n",
        "train_ds = TensorDataset(t_inputs, t_targets)\n",
        "for i in range(3):\n",
        "    print(train_ds[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nMFaa8suOd3"
      },
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "ZCsRo5Mx6wEI"
      },
      "outputs": [],
      "source": [
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymcQOo_hum6I"
      },
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "outputs": [],
      "source": [
        "class LogReg(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(3, 1)\n",
        "        self.act1 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.act1(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogReg()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RflV7xeVyoJy"
      },
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), 1e-5)\n",
        "loss_fn = F.binary_cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds = model(t_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'targets': t_targets.tolist(), 'predictions': preds.tolist()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>targets</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[1.0197797450928192e-07]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[5.324028995445929e-10]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[8.029338312167303e-17]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.0033887233585119247]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[5.971740029558914e-12]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[1.0197797450928192e-07]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[5.324028995445929e-10]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[8.029338312167303e-17]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.0033887233585119247]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[5.971740029558914e-12]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[1.0197797450928192e-07]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[5.324028995445929e-10]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[8.029338312167303e-17]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.0033887233585119247]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[5.971740029558914e-12]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   targets               predictions\n",
              "0    [0.0]  [1.0197797450928192e-07]\n",
              "1    [1.0]   [5.324028995445929e-10]\n",
              "2    [1.0]   [8.029338312167303e-17]\n",
              "3    [0.0]   [0.0033887233585119247]\n",
              "4    [1.0]   [5.971740029558914e-12]\n",
              "5    [0.0]  [1.0197797450928192e-07]\n",
              "6    [1.0]   [5.324028995445929e-10]\n",
              "7    [1.0]   [8.029338312167303e-17]\n",
              "8    [0.0]   [0.0033887233585119247]\n",
              "9    [1.0]   [5.971740029558914e-12]\n",
              "10   [0.0]  [1.0197797450928192e-07]\n",
              "11   [1.0]   [5.324028995445929e-10]\n",
              "12   [1.0]   [8.029338312167303e-17]\n",
              "13   [0.0]   [0.0033887233585119247]\n",
              "14   [1.0]   [5.971740029558914e-12]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_data = loss_fn(preds, t_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16.852367401123047\n"
          ]
        }
      ],
      "source": [
        "print(loss_data.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "На мою думку, модель не навчилася"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch-WrYnKzMzq"
      },
      "source": [
        "**Побудуємо функцію для навчання моделі на епохах та на датасетах**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "outputs": [],
      "source": [
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return {'losses': losses, 'model': model}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/1000], Loss: 15.4669\n",
            "Epoch [20/1000], Loss: 14.0212\n",
            "Epoch [30/1000], Loss: 12.6316\n",
            "Epoch [40/1000], Loss: 11.0121\n",
            "Epoch [50/1000], Loss: 9.4994\n",
            "Epoch [60/1000], Loss: 8.0390\n",
            "Epoch [70/1000], Loss: 6.6325\n",
            "Epoch [80/1000], Loss: 5.5362\n",
            "Epoch [90/1000], Loss: 4.9676\n",
            "Epoch [100/1000], Loss: 4.6565\n",
            "Epoch [110/1000], Loss: 4.4404\n",
            "Epoch [120/1000], Loss: 4.2666\n",
            "Epoch [130/1000], Loss: 4.1408\n",
            "Epoch [140/1000], Loss: 4.0378\n",
            "Epoch [150/1000], Loss: 3.9338\n",
            "Epoch [160/1000], Loss: 3.8486\n",
            "Epoch [170/1000], Loss: 3.7567\n",
            "Epoch [180/1000], Loss: 3.6718\n",
            "Epoch [190/1000], Loss: 3.5906\n",
            "Epoch [200/1000], Loss: 3.4966\n",
            "Epoch [210/1000], Loss: 3.4040\n",
            "Epoch [220/1000], Loss: 3.3215\n",
            "Epoch [230/1000], Loss: 3.2345\n",
            "Epoch [240/1000], Loss: 3.1515\n",
            "Epoch [250/1000], Loss: 3.0738\n",
            "Epoch [260/1000], Loss: 2.9838\n",
            "Epoch [270/1000], Loss: 2.9007\n",
            "Epoch [280/1000], Loss: 2.8183\n",
            "Epoch [290/1000], Loss: 2.7348\n",
            "Epoch [300/1000], Loss: 2.6602\n",
            "Epoch [310/1000], Loss: 2.5929\n",
            "Epoch [320/1000], Loss: 2.4941\n",
            "Epoch [330/1000], Loss: 2.4069\n",
            "Epoch [340/1000], Loss: 2.3264\n",
            "Epoch [350/1000], Loss: 2.2479\n",
            "Epoch [360/1000], Loss: 2.1748\n",
            "Epoch [370/1000], Loss: 2.0957\n",
            "Epoch [380/1000], Loss: 2.0343\n",
            "Epoch [390/1000], Loss: 1.9346\n",
            "Epoch [400/1000], Loss: 1.8578\n",
            "Epoch [410/1000], Loss: 1.7824\n",
            "Epoch [420/1000], Loss: 1.7074\n",
            "Epoch [430/1000], Loss: 1.6452\n",
            "Epoch [440/1000], Loss: 1.5661\n",
            "Epoch [450/1000], Loss: 1.4950\n",
            "Epoch [460/1000], Loss: 1.4271\n",
            "Epoch [470/1000], Loss: 1.3572\n",
            "Epoch [480/1000], Loss: 1.2863\n",
            "Epoch [490/1000], Loss: 1.2248\n",
            "Epoch [500/1000], Loss: 1.1634\n",
            "Epoch [510/1000], Loss: 1.1084\n",
            "Epoch [520/1000], Loss: 1.0582\n",
            "Epoch [530/1000], Loss: 0.9855\n",
            "Epoch [540/1000], Loss: 0.9415\n",
            "Epoch [550/1000], Loss: 0.8873\n",
            "Epoch [560/1000], Loss: 0.8405\n",
            "Epoch [570/1000], Loss: 0.7962\n",
            "Epoch [580/1000], Loss: 0.7537\n",
            "Epoch [590/1000], Loss: 0.7134\n",
            "Epoch [600/1000], Loss: 0.6804\n",
            "Epoch [610/1000], Loss: 0.6528\n",
            "Epoch [620/1000], Loss: 0.6165\n",
            "Epoch [630/1000], Loss: 0.5894\n",
            "Epoch [640/1000], Loss: 0.5642\n",
            "Epoch [650/1000], Loss: 0.5415\n",
            "Epoch [660/1000], Loss: 0.5204\n",
            "Epoch [670/1000], Loss: 0.5011\n",
            "Epoch [680/1000], Loss: 0.4879\n",
            "Epoch [690/1000], Loss: 0.4679\n",
            "Epoch [700/1000], Loss: 0.4531\n",
            "Epoch [710/1000], Loss: 0.4416\n",
            "Epoch [720/1000], Loss: 0.4287\n",
            "Epoch [730/1000], Loss: 0.4167\n",
            "Epoch [740/1000], Loss: 0.4053\n",
            "Epoch [750/1000], Loss: 0.3966\n",
            "Epoch [760/1000], Loss: 0.3855\n",
            "Epoch [770/1000], Loss: 0.3781\n",
            "Epoch [780/1000], Loss: 0.3708\n",
            "Epoch [790/1000], Loss: 0.3640\n",
            "Epoch [800/1000], Loss: 0.3568\n",
            "Epoch [810/1000], Loss: 0.3543\n",
            "Epoch [820/1000], Loss: 0.3437\n",
            "Epoch [830/1000], Loss: 0.3376\n",
            "Epoch [840/1000], Loss: 0.3325\n",
            "Epoch [850/1000], Loss: 0.3308\n",
            "Epoch [860/1000], Loss: 0.3231\n",
            "Epoch [870/1000], Loss: 0.3191\n",
            "Epoch [880/1000], Loss: 0.3150\n",
            "Epoch [890/1000], Loss: 0.3102\n",
            "Epoch [900/1000], Loss: 0.3067\n",
            "Epoch [910/1000], Loss: 0.3047\n",
            "Epoch [920/1000], Loss: 0.2996\n",
            "Epoch [930/1000], Loss: 0.2968\n",
            "Epoch [940/1000], Loss: 0.2930\n",
            "Epoch [950/1000], Loss: 0.2901\n",
            "Epoch [960/1000], Loss: 0.2873\n",
            "Epoch [970/1000], Loss: 0.2850\n",
            "Epoch [980/1000], Loss: 0.2829\n",
            "Epoch [990/1000], Loss: 0.2796\n",
            "Epoch [1000/1000], Loss: 0.2780\n"
          ]
        }
      ],
      "source": [
        "loss = fit_return_loss(1000, model, loss_fn, optimizer, train_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "fitted_model = loss['model']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>targets</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.5415476560592651]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.7994722127914429]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.7863525152206421]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.09837473183870316]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.9633612632751465]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.5415476560592651]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.7994722127914429]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.7863525152206421]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.09837473183870316]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.9633612632751465]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.5415476560592651]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.7994722127914429]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.7863525152206421]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.09837473183870316]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.9633612632751465]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   targets            predictions\n",
              "0    [0.0]   [0.5415476560592651]\n",
              "1    [1.0]   [0.7994722127914429]\n",
              "2    [1.0]   [0.7863525152206421]\n",
              "3    [0.0]  [0.09837473183870316]\n",
              "4    [1.0]   [0.9633612632751465]\n",
              "5    [0.0]   [0.5415476560592651]\n",
              "6    [1.0]   [0.7994722127914429]\n",
              "7    [1.0]   [0.7863525152206421]\n",
              "8    [0.0]  [0.09837473183870316]\n",
              "9    [1.0]   [0.9633612632751465]\n",
              "10   [0.0]   [0.5415476560592651]\n",
              "11   [1.0]   [0.7994722127914429]\n",
              "12   [1.0]   [0.7863525152206421]\n",
              "13   [0.0]  [0.09837473183870316]\n",
              "14   [1.0]   [0.9633612632751465]"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_preds = fitted_model(t_inputs)\n",
        "dff = pd.DataFrame({'targets': t_targets.tolist(), 'predictions': final_preds.tolist()})\n",
        "dff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Лосс зменшується, модель навчилася гарно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss['losses'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Тут я б виділила два \"ліктя\" - на 120 епохах (більш виражений) та на 650 епохах"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
